{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import utils\n",
    "from utils import HELOC_NAME, COVERTYPE_NAME, CALIFORNIA_HOUSING_NAME, DENGUE_DATASET\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DROPOUT_VALUES = [0.2, 0.5]\n",
    "\n",
    "BATCH_SIZES = {\n",
    "    HELOC_NAME: 64,\n",
    "    COVERTYPE_NAME: 256,\n",
    "    CALIFORNIA_HOUSING_NAME: 64, \n",
    "    DENGUE_DATASET: 64,\n",
    "}\n",
    "EPOCHS = 200\n",
    "\n",
    "METRICS_CLASSIFICATION_BINARY = [\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall(),\n",
    "    tf.keras.metrics.BinaryAccuracy()\n",
    "]\n",
    "\n",
    "METRICS_CLASSIFICATION_MULTICLASS = [\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall(),\n",
    "    tf.keras.metrics.CategoricalAccuracy()\n",
    "]\n",
    "\n",
    "METRICS_REGRESSION = [\n",
    "    tf.keras.metrics.MeanSquaredError(),\n",
    "    tf.keras.metrics.MeanAbsoluteError()\n",
    "]\n",
    "\n",
    "MIDDLE_ACTIVATIONS = [\"sigmoid\", \"relu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, InputLayer, LayerNormalization\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Activation,MaxPooling2D, AveragePooling2D, Concatenate\n",
    "\n",
    "def build_model(input_shape, is_classification_dataset, dropout_value, middle_activation, last_layer_n, n_classes=None):\n",
    "    if is_classification_dataset:\n",
    "        assert n_classes is not None\n",
    "        assert n_classes > 1\n",
    "    else:\n",
    "        assert n_classes is None\n",
    "\n",
    "    # Define the model architecture\n",
    "\n",
    "    #Entrada\n",
    "    input_shape = Input(input_shape)\n",
    "\n",
    "    #Inicio de rama 1\n",
    "    tower_1 = Conv2D(16, (3,3), activation='relu',padding=\"same\")(input_shape)\n",
    "    tower_1 = BatchNormalization()(tower_1)\n",
    "    tower_1 = Activation('relu')(tower_1)\n",
    "    tower_1 = MaxPooling2D(2,2)(tower_1)\n",
    "    tower_1 = Dropout(dropout_value)(tower_1)\n",
    "\n",
    "    tower_1 = Conv2D(32, (3,3), activation='relu',padding=\"same\")(tower_1)\n",
    "    tower_1 = BatchNormalization()(tower_1)\n",
    "    tower_1 = Activation('relu')(tower_1)\n",
    "    tower_1 = MaxPooling2D(2,2)(tower_1)\n",
    "    tower_1 = Dropout(dropout_value)(tower_1)\n",
    "\n",
    "    tower_1 = Conv2D(64, (3,3), activation='relu',padding=\"same\")(tower_1)\n",
    "    tower_1 = BatchNormalization()(tower_1)\n",
    "    tower_1 = Activation('relu')(tower_1)\n",
    "    tower_1 = MaxPooling2D(2,2)(tower_1)\n",
    "    tower_1 = Dropout(dropout_value)(tower_1)\n",
    "\n",
    "    tower_1 = Conv2D(last_layer_n, (3,3), activation='relu',padding=\"same\")(tower_1)\n",
    "    tower_1 = BatchNormalization()(tower_1)\n",
    "    tower_1 = Activation('relu')(tower_1)\n",
    "    tower_1 = MaxPooling2D(2,2)(tower_1)\n",
    "    tower_1 = Dropout(dropout_value)(tower_1)\n",
    "    #Fin de rama 1\n",
    "\n",
    "    #Inicio de rama 2\n",
    "    tower_2 = Conv2D(16, (5,5), activation='relu',padding=\"same\")(input_shape)\n",
    "    tower_2 = BatchNormalization()(tower_2)\n",
    "    tower_2 = Activation('relu')(tower_2)\n",
    "    tower_2 = AveragePooling2D(2,2)(tower_2)\n",
    "    tower_2 = Dropout(dropout_value)(tower_2)\n",
    "\n",
    "    tower_2 = Conv2D(32, (5,5), activation='relu',padding=\"same\")(tower_2)\n",
    "    tower_2 = BatchNormalization()(tower_2)\n",
    "    tower_2 = Activation('relu')(tower_2)\n",
    "    tower_2 = AveragePooling2D(2,2)(tower_2)\n",
    "    tower_2 = Dropout(dropout_value)(tower_2)\n",
    "\n",
    "    tower_2 = Conv2D(64, (5,5), activation='relu',padding=\"same\")(tower_2)\n",
    "    tower_2 = BatchNormalization()(tower_2)\n",
    "    tower_2 = Activation('relu')(tower_2)\n",
    "    tower_2 = AveragePooling2D(2,2)(tower_2)\n",
    "    tower_2 = Dropout(dropout_value)(tower_2)\n",
    "\n",
    "    tower_2 = Conv2D(last_layer_n, (5,5), activation='relu',padding=\"same\")(tower_2)\n",
    "    tower_2 = BatchNormalization()(tower_2)\n",
    "    tower_2 = Activation('relu')(tower_2)\n",
    "    tower_2 = AveragePooling2D(2,2)(tower_2)\n",
    "    tower_2 = Dropout(dropout_value)(tower_2)\n",
    "    #Fin de rama 2\n",
    "\n",
    "    #Concatenación de las 2 ramas\n",
    "    merged = Concatenate(axis=1)([tower_1, tower_2])\n",
    "\n",
    "    #Aplanamiento\n",
    "    merged = Flatten()(merged)\n",
    "\n",
    "    #Capas adicionales\n",
    "    out = Dense(256, activation='relu')(merged)\n",
    "    out = Dense(128, activation=middle_activation)(out)\n",
    "    out = Dense(64, activation=middle_activation)(out)\n",
    "    out = Dense(32, activation='sigmoid')(out)\n",
    "\n",
    "    #Capa final de clasificación\n",
    "    if is_classification_dataset:\n",
    "        if n_classes == 2:\n",
    "            out = Dense(1, activation='sigmoid')(out)\n",
    "        else:\n",
    "            out = Dense(n_classes, activation='softmax')(out)\n",
    "    else:\n",
    "        out = Dense(1, activation=\"linear\")(out)\n",
    "\n",
    "    model = Model(input_shape, out)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer = \"adam\",\n",
    "        metrics = METRICS_REGRESSION if not is_classification_dataset else \\\n",
    "            (METRICS_CLASSIFICATION_BINARY if n_classes == 2 else METRICS_CLASSIFICATION_MULTICLASS),\n",
    "        loss = \"mean_squared_error\" if not is_classification_dataset else \\\n",
    "            (\"binary_crossentropy\" if n_classes == 2 else \"categorical_crossentropy\")\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    X_train,\n",
    "    X_val,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    batch_size,\n",
    "    classes_weight\n",
    "):\n",
    "    model.fit(\n",
    "        x = X_train,\n",
    "        y = y_train,\n",
    "        validation_data = (X_val, y_val),\n",
    "        batch_size = batch_size,\n",
    "        epochs = EPOCHS,\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss',\n",
    "                patience = 20,\n",
    "                restore_best_weights = True,\n",
    "                start_from_epoch = 10\n",
    "            )\n",
    "        ],\n",
    "        class_weight = classes_weight\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partial_load_image(image_method_folder):\n",
    "    def load_image(img_path):\n",
    "        return cv2.imread(\n",
    "            os.path.join(image_method_folder, img_path)\n",
    "        )\n",
    "    \n",
    "    return load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_dataset(dataset_name):\n",
    "    dataset_folder = utils.get_cnnmodels_path(dataset_name)\n",
    "\n",
    "    # Get all available image methods\n",
    "    available_images = {\n",
    "        image_method\n",
    "        for image_method in utils.ALL_IMAGE_METHODS\n",
    "        if any(\n",
    "            f.endswith(\"csv\") for f in os.listdir(utils.get_images_path_for_dataset(dataset_name, image_method))\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Get all already done image methods\n",
    "    finished_methods = set([f.split(\"_\")[0] for f in os.listdir(dataset_folder)])\n",
    "\n",
    "    # Get all remaining methods\n",
    "    remaining_methods = available_images - finished_methods\n",
    "\n",
    "    if not remaining_methods:\n",
    "        return\n",
    "\n",
    "    # Get the dataset\n",
    "    X,y = utils.get_X_y(dataset_name)\n",
    "    del X   # In this experiment I use images (not the raw data)\n",
    "\n",
    "    # Get the indices for train and validation split\n",
    "    indices_train,indices_val = utils.get_indices_train_eval(dataset_name)\n",
    "    y_train = y[indices_train]\n",
    "    y_val = y[indices_val]\n",
    "\n",
    "    # Get the class weights\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    equal_weights = compute_class_weight(class_weight=None, classes=classes, y=y_train)\n",
    "\n",
    "    class_weight_dict = {classes[i]: weight for i, weight in enumerate(class_weights)}\n",
    "    equal_weight_dict = {classes[i]: weight for i, weight in enumerate(equal_weights)}\n",
    "\n",
    "    if utils.is_dataset_classification(dataset_name):\n",
    "        is_classification_dataset = True\n",
    "        n_classes = utils.get_number_of_classes(dataset_name)\n",
    "        assert n_classes > 1\n",
    "    else:\n",
    "        is_classification_dataset = False\n",
    "        n_classes = None\n",
    "\n",
    "    if utils.is_dataset_multiclass_classification(dataset_name):\n",
    "        y_train = to_categorical(y_train, num_classes=n_classes)\n",
    "        y_val = to_categorical(y_val, num_classes=n_classes)\n",
    "    del y\n",
    "\n",
    "    for image_method in sorted(remaining_methods):\n",
    "        image_method_folder = utils.get_images_path_for_dataset(dataset_name, image_method)\n",
    "\n",
    "        # Load the routes to the images\n",
    "        csv_file_path = os.path.join(\n",
    "            image_method_folder,\n",
    "            next(f for f in os.listdir(image_method_folder) if f.endswith(\".csv\"))\n",
    "        )\n",
    "        image_paths = pd.read_csv(csv_file_path)\n",
    "\n",
    "        func_load_image = get_partial_load_image(image_method_folder)\n",
    "\n",
    "        image_paths_np = image_paths[\"images\"].to_numpy()\n",
    "        train_paths = image_paths_np[indices_train]\n",
    "        val_paths = image_paths_np[indices_val]\n",
    "\n",
    "        del image_paths\n",
    "\n",
    "        # Load train and validation images\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            X_train = np.array(list(executor.map(func_load_image, train_paths)))\n",
    "            X_val = np.array(list(executor.map(func_load_image, val_paths)))\n",
    "\n",
    "        for dropout_value in DROPOUT_VALUES:\n",
    "            for middle_activation in MIDDLE_ACTIVATIONS:\n",
    "                for balancing_classes in [False, True]:\n",
    "                    for last_layer_n in [128, 64]:\n",
    "                        print(image_method, dropout_value, middle_activation, balancing_classes)\n",
    "\n",
    "                        # Build the model\n",
    "                        model = build_model(\n",
    "                            input_shape = X_train[0].shape,\n",
    "                            is_classification_dataset = is_classification_dataset,\n",
    "                            dropout_value = dropout_value,\n",
    "                            middle_activation = middle_activation,\n",
    "                            last_layer_n = last_layer_n,\n",
    "                            n_classes = n_classes,\n",
    "                        )\n",
    "                        \n",
    "                        # Train the model\n",
    "                        train_model(\n",
    "                            model = model,\n",
    "                            X_train = X_train,\n",
    "                            X_val = X_val,\n",
    "                            y_train = y_train,\n",
    "                            y_val = y_val,\n",
    "                            batch_size = BATCH_SIZES[dataset_name],\n",
    "                            classes_weight = class_weight_dict if balancing_classes else equal_weight_dict\n",
    "                        )\n",
    "\n",
    "                        # Save the model\n",
    "                        model.save(os.path.join(dataset_folder, f\"{image_method}_{middle_activation}_{dropout_value}_balance={balancing_classes}.keras\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HELOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_dataset(HELOC_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DENGUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_dataset(DENGUE_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_dataset(CALIFORNIA_HOUSING_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_dataset(COVERTYPE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
