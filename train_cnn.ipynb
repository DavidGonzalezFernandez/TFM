{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import utils\n",
    "from utils import HELOC_NAME, ADULT_INCOME_NAME, HIGGS_NAME, COVERTYPE_NAME, CALIFORNIA_HOUSING_NAME, ARBOVIRUSES_NAME\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DROPOUT_VALUE = 0.5\n",
    "\n",
    "BATCH_SIZES = {\n",
    "    HELOC_NAME: 64,\n",
    "    ADULT_INCOME_NAME: 64, \n",
    "    HIGGS_NAME: 4096,\n",
    "    COVERTYPE_NAME: 256,\n",
    "    CALIFORNIA_HOUSING_NAME: 64, \n",
    "    ARBOVIRUSES_NAME: 64,\n",
    "}\n",
    "EPOCHS = 250\n",
    "\n",
    "METRICS_CLASSIFICATION_BINARY = [\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall(),\n",
    "    tf.keras.metrics.BinaryAccuracy()\n",
    "]\n",
    "\n",
    "METRICS_CLASSIFICATION_MULTICLASS = [\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall(),\n",
    "    tf.keras.metrics.CategoricalAccuracy()\n",
    "]\n",
    "\n",
    "METRICS_REGRESSION = [\n",
    "    tf.keras.metrics.MeanSquaredError(),\n",
    "    tf.keras.metrics.MeanAbsoluteError()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import vgg16, vgg19, resnet50, mobilenet, inception_resnet_v2, densenet, inception_v3, xception, nasnet, ResNet152V2\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, InputLayer, LayerNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adamax\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanAbsolutePercentageError\n",
    "from tensorflow.keras.layers import Input, Activation,MaxPooling2D, concatenate, AveragePooling2D, Concatenate\n",
    "\n",
    "def build_model(input_shape, is_classification_dataset, n_classes=None):\n",
    "    if is_classification_dataset:\n",
    "        assert n_classes is not None\n",
    "        assert n_classes > 1\n",
    "    else:\n",
    "        assert n_classes is None\n",
    "\n",
    "    # Define the model architecture\n",
    "\n",
    "    #Entrada\n",
    "    input_shape = Input(input_shape)\n",
    "\n",
    "    #Inicio de rama 1\n",
    "    tower_1 = Conv2D(16, (3,3), activation='relu',padding=\"same\")(input_shape)\n",
    "    tower_1 = BatchNormalization()(tower_1)\n",
    "    tower_1 = Activation('relu')(tower_1)\n",
    "    # tower_1 = MaxPooling2D(2,2)(tower_1)\n",
    "    tower_1 = Dropout(DROPOUT_VALUE)(tower_1)\n",
    "\n",
    "    tower_1 = Conv2D(32, (3,3), activation='relu',padding=\"same\")(tower_1)\n",
    "    tower_1 = BatchNormalization()(tower_1)\n",
    "    tower_1 = Activation('relu')(tower_1)\n",
    "    # tower_1 = MaxPooling2D(2,2)(tower_1)\n",
    "    tower_1 = Dropout(DROPOUT_VALUE)(tower_1)\n",
    "\n",
    "    tower_1 = Conv2D(64, (3,3), activation='relu',padding=\"same\")(tower_1)\n",
    "    tower_1 = BatchNormalization()(tower_1)\n",
    "    tower_1 = Activation('relu')(tower_1)\n",
    "    # tower_1 = MaxPooling2D(2,2)(tower_1)\n",
    "    tower_1 = Dropout(DROPOUT_VALUE)(tower_1)\n",
    "\n",
    "    tower_1 = Conv2D(64, (3,3), activation='relu',padding=\"same\")(tower_1)\n",
    "    tower_1 = BatchNormalization()(tower_1)\n",
    "    tower_1 = Activation('relu')(tower_1)\n",
    "    # tower_1 = MaxPooling2D(2,2)(tower_1)\n",
    "    tower_1 = Dropout(DROPOUT_VALUE)(tower_1)\n",
    "    #Fin de rama 1\n",
    "\n",
    "    #Inicio de rama 2\n",
    "    tower_2 = Conv2D(16, (5,5), activation='relu',padding=\"same\")(input_shape)\n",
    "    tower_2 = BatchNormalization()(tower_2)\n",
    "    tower_2 = Activation('relu')(tower_2)\n",
    "    # tower_2 = AveragePooling2D(2,2)(tower_2)\n",
    "    tower_2 = Dropout(DROPOUT_VALUE)(tower_2)\n",
    "\n",
    "    tower_2 = Conv2D(32, (5,5), activation='relu',padding=\"same\")(tower_2)\n",
    "    tower_2 = BatchNormalization()(tower_2)\n",
    "    tower_2 = Activation('relu')(tower_2)\n",
    "    # tower_2 = AveragePooling2D(2,2)(tower_2)\n",
    "    tower_2 = Dropout(DROPOUT_VALUE)(tower_2)\n",
    "\n",
    "    tower_2 = Conv2D(64, (5,5), activation='relu',padding=\"same\")(tower_2)\n",
    "    tower_2 = BatchNormalization()(tower_2)\n",
    "    tower_2 = Activation('relu')(tower_2)\n",
    "    # tower_2 = AveragePooling2D(2,2)(tower_2)\n",
    "    tower_2 = Dropout(DROPOUT_VALUE)(tower_2)\n",
    "\n",
    "    tower_2 = Conv2D(64, (5,5), activation='relu',padding=\"same\")(tower_2)\n",
    "    tower_2 = BatchNormalization()(tower_2)\n",
    "    tower_2 = Activation('relu')(tower_2)\n",
    "    # tower_2 = AveragePooling2D(2,2)(tower_2)\n",
    "    tower_2 = Dropout(DROPOUT_VALUE)(tower_2)\n",
    "    #Fin de rama 2\n",
    "\n",
    "    #Concatenación de las 2 ramas\n",
    "    merged = Concatenate(axis=1)([tower_1, tower_2])\n",
    "\n",
    "    #Aplanamiento\n",
    "    merged = Flatten()(merged)\n",
    "\n",
    "    #Capas adicionales\n",
    "    out = Dense(256, activation='relu')(merged)\n",
    "    out = Dense(128, activation='sigmoid')(out)\n",
    "    out = Dense(64, activation='sigmoid')(out)\n",
    "    out = Dense(32, activation='sigmoid')(out)\n",
    "\n",
    "    #Capa final de clasificación\n",
    "    if is_classification_dataset:\n",
    "        if n_classes == 2:\n",
    "            out = Dense(1, activation='sigmoid')(out)\n",
    "        else:\n",
    "            out = Dense(n_classes, activation='softmax')(out)\n",
    "    else:\n",
    "        out = Dense(1, activation=\"linear\")(out)\n",
    "\n",
    "    model = Model(input_shape, out)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer = \"adam\",\n",
    "        metrics = METRICS_REGRESSION if not is_classification_dataset else \\\n",
    "            (METRICS_CLASSIFICATION_BINARY if n_classes == 2 else METRICS_CLASSIFICATION_MULTICLASS),\n",
    "        loss = \"mean_squared_error\" if not is_classification_dataset else \\\n",
    "            (\"binary_crossentropy\" if n_classes == 2 else \"categorical_crossentropy\")\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    X_train,\n",
    "    X_val,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    batch_size\n",
    "):\n",
    "    model.fit(\n",
    "        x = X_train,\n",
    "        y = y_train,\n",
    "        validation_data = (X_val, y_val),\n",
    "        batch_size = batch_size,\n",
    "        epochs = EPOCHS,\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss',\n",
    "                patience = 20,\n",
    "                restore_best_weights = True,\n",
    "                start_from_epoch = 20\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_FOLDER = \"images\"\n",
    "\n",
    "CNN_RESULTS_FOLDER = \"cnn_models\"\n",
    "RESULT_FOLDER = os.path.join(utils.get_results_path(), CNN_RESULTS_FOLDER)\n",
    "if not os.path.exists(RESULT_FOLDER):\n",
    "    os.mkdir(RESULT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partial_load_image(image_method_folder):\n",
    "    def load_image(img_path):\n",
    "        return cv2.imread(\n",
    "            os.path.join(image_method_folder, img_path)\n",
    "        )\n",
    "    \n",
    "    return load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_dataset(dataset_name):\n",
    "    dataset_folder = os.path.join(RESULT_FOLDER, dataset_name)\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        os.mkdir(dataset_folder)\n",
    "\n",
    "    images_folder = utils.get_images_path()\n",
    "    # Get all available image methods\n",
    "    available_images = set([\n",
    "        d for d in os.listdir(images_folder)\n",
    "        if\n",
    "            (\"supervised.csv\" if utils.is_dataset_classification(dataset_name) else \"regression.csv\")\n",
    "            in os.listdir(os.path.join(images_folder, d, dataset_name.lower()))\n",
    "    ])\n",
    "\n",
    "    # Get all already done image methods\n",
    "    finished_methods = set([\n",
    "        f.split(\".\")[0] for f in os.listdir(dataset_folder)\n",
    "    ])\n",
    "    # Get all remaining methods\n",
    "    remaining_methods = available_images - finished_methods\n",
    "\n",
    "    if not remaining_methods:\n",
    "        return\n",
    "\n",
    "    # Get the dataset\n",
    "    X,y = utils.get_X_y(dataset_name)\n",
    "    del X   # In this experiment I use images (not the raw data)\n",
    "\n",
    "    # Get the indices for train and validation split\n",
    "    indices_train,indices_val = utils.get_indices_train_eval(dataset_name)\n",
    "    y_train = y[indices_train]\n",
    "    y_val = y[indices_val]\n",
    "\n",
    "    if utils.is_dataset_classification(dataset_name):\n",
    "        is_classification_dataset = True\n",
    "        n_classes = np.unique(y).shape[0]\n",
    "        assert n_classes > 1\n",
    "    else:\n",
    "        is_classification_dataset = False\n",
    "        n_classes = None\n",
    "\n",
    "    if utils.is_dataset_multiclass_classification(dataset_name):\n",
    "        y_train = to_categorical(y_train, num_classes=n_classes)\n",
    "        y_val = to_categorical(y_val, num_classes=n_classes)\n",
    "    del y\n",
    "\n",
    "    for image_method in sorted(remaining_methods):\n",
    "        print(image_method)\n",
    "        image_method_folder = os.path.join(images_folder, image_method, dataset_name.lower())\n",
    "\n",
    "        # Load the routes to the images\n",
    "        csv_file_path = os.path.join(image_method_folder, (\"supervised.csv\" if utils.is_dataset_classification(dataset_name) else \"regression.csv\"))\n",
    "        image_paths = pd.read_csv(csv_file_path)\n",
    "\n",
    "        func_load_image = get_partial_load_image(image_method_folder)\n",
    "\n",
    "        image_paths_np = image_paths[\"images\"].to_numpy()\n",
    "        train_paths = image_paths_np[indices_train]\n",
    "        val_paths = image_paths_np[indices_val]\n",
    "\n",
    "        del image_paths\n",
    "\n",
    "        # Load train and validation images\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            X_train = np.array(list(executor.map(func_load_image, train_paths)))\n",
    "            X_val = np.array(list(executor.map(func_load_image, val_paths)))\n",
    "\n",
    "        # Build the model\n",
    "        model = build_model(\n",
    "            input_shape = X_train[0].shape,\n",
    "            is_classification_dataset = is_classification_dataset,\n",
    "            n_classes = n_classes\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        train_model(\n",
    "            model = model,\n",
    "            X_train = X_train,\n",
    "            X_val = X_val,\n",
    "            y_train = y_train,\n",
    "            y_val = y_val,\n",
    "            batch_size = BATCH_SIZES[dataset_name]\n",
    "        )\n",
    "\n",
    "        # Save the model\n",
    "        model.save(os.path.join(dataset_folder, f\"{image_method}.keras\"))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HELOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_dataset(HELOC_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_dataset(ADULT_INCOME_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_dataset(CALIFORNIA_HOUSING_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arboviruses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_dataset(ARBOVIRUSES_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_dataset(COVERTYPE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIGGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_with_dataset(HIGGS_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
